from pyspark.sql import SparkSession
from pyspark.sql.functions import avg
import time

# Start Spark session
spark = SparkSession.builder.appName("PerformanceTest").getOrCreate()

# List of data files in GCS
files = ["data_1.csv", "data_2.csv", "data_3.csv", "data_4.csv", "data_5.csv"]
runtimes = []

# Run Spark aggregation and measure time
for file in files:
    print(f"Processing {file}...")
    df = spark.read.option("header", True).csv(f"file:///home/jupyter/GCS/{file}")
    start = time.time()
    df.groupBy("Country").agg(avg("Index")).show()
    end = time.time()
    runtimes.append(end - start)

# Stop Spark session
spark.stop()

# Print results
print("Runtimes (seconds):", runtimes)
